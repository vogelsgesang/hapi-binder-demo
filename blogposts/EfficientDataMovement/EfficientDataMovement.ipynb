{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Data Movement using the HyperAPI\n",
    "\n",
    "*Author: Adrian Vogelsgesang*\n",
    "\n",
    "The latest HyperAPI allows you to not only write data into Hyper files but also to read data from Hyper files. Thereby it enables a few new use cases.\n",
    "\n",
    "The use case we will focus on in this blog post:\n",
    "you can now union Hyper files directly using the API.\n",
    "If you aren't careful though, you will end up with a subpar solution which is way slower than necessary.\n",
    "\n",
    "In this blog post, we are going to show you the optimal way for unioning Hyper files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The scenario\n",
    "\n",
    "We are going to use the well-known \"World Indicators\" data set which ships as part of the \"World Indicators\" demo workbook with Tableau.\n",
    "\n",
    "However, somehow, our data ended up split into multiple files: Instead of one \"WorldIndicators.hyper\", we have 12 of the them:\n",
    "\n",
    "* WorldIndicators_2000.hyper\n",
    "* WorldIndicators_2001.hyper\n",
    "* WorldIndicators_2002.hyper\n",
    "* ...\n",
    "* WorldIndicators_2012.hyper\n",
    "\n",
    "and we would like to combine all of those together into just one Hyper file.\n",
    "\n",
    "In this little blog post, we will be using Python.\n",
    "If you want to follow along, please [download and install the Hyper API for Python](Test). Next, [download the input files](Test2) and unzip them.  **TODO: links need to be updated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The inefficient solution: Processing the rows in Python\n",
    "\n",
    "In this section, we will first take a look at a correct but slow solution.\n",
    "This is *not* the recommended way to union your data.\n",
    "It still shows some valuable tricks (such as copying over a table definition from one file into another file).\n",
    "If you are just here looking for some code to copy & paste, you can skip this section.\n",
    "\n",
    "So, let's get started: First, let's find all the Hyper files we want to combine. We can use a simple glob pattern for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "input_files = glob(\"WorldIndicators_*.hyper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore we need to know from which table within the Hyper file we want to read the data from. Let's simply declare a global variable for this for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tableauhyperapi import HyperProcess, Connection, Telemetry, TableDefinition, TableName, SchemaName, Inserter, CreateMode\n",
    "\n",
    "table_name = TableName('Extract','Extract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go over all those files and read all the data out of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data out of all files\n",
    "# DON'T DO THIS! This is an counterexample which shows how not to write this code.\n",
    "# Further down, we will show you how to do this more efficiently\n",
    "unioned_data = []\n",
    "\n",
    "# Start a new Hyper instance\n",
    "with HyperProcess(Telemetry.SEND_USAGE_DATA_TO_TABLEAU, 'unionfiles_inefficient') as hyper:\n",
    "    # Go over all the input files\n",
    "    for file in input_files:\n",
    "        # Connect to the Hyper file and read all data from it\n",
    "        with Connection(hyper.endpoint, database=file) as connection:\n",
    "            unioned_data += connection.execute_list_query(f\"SELECT * FROM {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we are using `SELECT *` instead of listing the column names individually. Thereby, we can use this script against arbitrary Hyper files, no matter how the columns are actually called.\n",
    "\n",
    "The query is composed using string operations, namely the f-string feature of Python.\n",
    "Seeing how we build this query out of string operations, one might become scared of SQL injections.\n",
    "Luckily for us, table_name is a `QualifiedName` and we made sure that its `to_string` method takes care of proper quoting.\n",
    "\n",
    "Still, in case you don't know what SQL injections are, I urge you to [read up on it](https://arstechnica.com/information-technology/2016/10/how-security-flaws-work-sql-injection/). SQL injections are among the most common security vulnerabilities.\n",
    "\n",
    "Now that we have all our data in the `unioned_data` list, we want to store it into a new file.\n",
    "Before we can insert the data into a new file, we first need a table within that file, though.\n",
    "This table should have the same columns and column types as our input tables.\n",
    "For simplicity, let's just assume that all our input files have the same columns (otherwise we would fail, anyway) and just duplicate the table definition from the first input file into our output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file=\"WorldIndicatorsMerged.hyper\"\n",
    "with HyperProcess(Telemetry.SEND_USAGE_DATA_TO_TABLEAU, 'unionfiles_inefficient') as hyper:\n",
    "    # Get the table's schema from the first input file.\n",
    "    # We will just assume that all files have the exact same schema.\n",
    "    with Connection(hyper.endpoint, database=input_files[0]) as connection:\n",
    "        table_def = connection.catalog.get_table_definition(table_name)\n",
    "    # We need to reset the table_def.table_name since the TableDefinition returned by\n",
    "    # `catalog.get_table_definition` also contains the database name of the\n",
    "    # database we executed it against. E.g., it reads \"WorldIndicators2017.Extract.Extract\".\n",
    "    # We want to recreate the table in a different database, though, and hence we reset it\n",
    "    # to \"Extract.Extract\".\n",
    "    table_def.table_name = table_name\n",
    "    # Create the same table in the target database\n",
    "    with Connection(hyper.endpoint, database=output_file, create_mode=CreateMode.CREATE_AND_REPLACE) as connection:\n",
    "        # Create the output table\n",
    "        connection.catalog.create_schema(SchemaName(table_name.schema_name))\n",
    "        connection.catalog.create_table(table_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, this is straightforward:\n",
    "We get the table definition from the first input table and then pass it to `create_table` to create the same table in the output file.\n",
    "\n",
    "However, there are two stumbling blocks on our way:\n",
    "\n",
    "1. We need to reset the `table_def.table_name` since the table definition returned by\n",
    "  `catalog.get_table_definition` also contains the database name of the\n",
    "  database we executed it against. E.g., it reads \"WorldIndicators2017.Extract.Extract\".\n",
    "  We want to recreate the table in a different database, though, and hence we reset it\n",
    "  to \"Extract.Extract\".\n",
    "2. We first need to create the schema \"Extract\" before we can create a table within it.\n",
    "\n",
    "With that out of the way, all that remains to be done is to insert the data into our new table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with HyperProcess(Telemetry.SEND_USAGE_DATA_TO_TABLEAU, 'unionfiles_inefficient') as hyper:\n",
    "    with Connection(hyper.endpoint, database=output_file) as connection:\n",
    "        # Insert the data\n",
    "        with Inserter(connection, table_def) as inserter:\n",
    "            inserter.add_rows(unioned_data)\n",
    "            inserter.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we can simply pass the list of unioned tuples into `add_rows`. No need to add them one by one.\n",
    "\n",
    "And voila, we have our unioned output file ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "However, this approach has one large obvious inefficiency:\n",
    "We restarted Hyper multiple times and reconnected more often than necessary.\n",
    "Let's quickly fix that by restructuring our code.\n",
    "While at it, let's also add a few timing outputs to get a feeling for how fast our code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21025586128234863: Reading WorldIndicators_2007.hyper\n",
      "0.27318382263183594: Reading WorldIndicators_2011.hyper\n",
      "0.33381152153015137: Reading WorldIndicators_2002.hyper\n",
      "0.38861918449401855: Reading WorldIndicators_2004.hyper\n",
      "0.44709086418151855: Reading WorldIndicators_2009.hyper\n",
      "0.5095717906951904: Reading WorldIndicators_2008.hyper\n",
      "0.5679950714111328: Reading WorldIndicators_2010.hyper\n",
      "0.626805305480957: Reading WorldIndicators_2001.hyper\n",
      "0.687401294708252: Reading WorldIndicators_2012.hyper\n",
      "0.7462217807769775: Reading WorldIndicators_2005.hyper\n",
      "0.8062338829040527: Reading WorldIndicators_2003.hyper\n",
      "0.8644604682922363: Reading WorldIndicators_2000.hyper\n",
      "0.9244029521942139: Reading WorldIndicators_2006.hyper\n",
      "0.9844169616699219: Inserting data...\n",
      "1.2255094051361084: Done :)\n"
     ]
    }
   ],
   "source": [
    "from tableauhyperapi import HyperProcess, Connection, Telemetry, TableDefinition, TableName, SchemaName, Inserter, CreateMode\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "input_files = glob(\"WorldIndicators_*.hyper\")\n",
    "table_name = TableName('Extract','Extract')\n",
    "output_file = \"WorldIndicatorsMerged.hyper\"\n",
    "\n",
    "# Start a new Hyper instance\n",
    "start_time = time()\n",
    "with HyperProcess(Telemetry.SEND_USAGE_DATA_TO_TABLEAU, 'unionfiles_inefficient') as hyper:\n",
    "    # Get the table's schema from the first input file.\n",
    "    # We will just assume that all files have the exact same schema.\n",
    "    with Connection(hyper.endpoint, database=input_files[0]) as connection:\n",
    "        table_def = connection.catalog.get_table_definition(table_name)\n",
    "    # Read the data out of all files\n",
    "    # DON'T DO THIS! This is an counterexample which shows how not to write this code\n",
    "    unioned_data = []\n",
    "    for file in input_files:\n",
    "        # Some poor-man's tracing, so we can see how we make progress\n",
    "        print(f\"{time() - start_time}: Reading {file}\")\n",
    "        # Connect to the Hyper file and read all data from it\n",
    "        with Connection(hyper.endpoint, database=file) as connection:\n",
    "            unioned_data += connection.execute_list_query(f\"SELECT * FROM {table_name}\")\n",
    "    # Create the output file and insert the data\n",
    "    print (f\"{time() - start_time}: Inserting data...\")\n",
    "    with Connection(hyper.endpoint, database=output_file, create_mode=CreateMode.CREATE_AND_REPLACE) as connection:\n",
    "        # We need to reset the table_def.table_name since the TableDefinition returned by\n",
    "        # `catalog.get_table_definition` also contains the database name of the\n",
    "        # database we executed it against. E.g., it reads \"WorldIndicators2017.Extract.Extract\".\n",
    "        # We want to recreate the table in a different database, though, and hence we reset it\n",
    "        # to \"Extract.Extract\".\n",
    "        table_def.table_name = table_name\n",
    "        # Create the output table\n",
    "        connection.catalog.create_schema(SchemaName(table_name.schema_name))\n",
    "        connection.catalog.create_table(table_def)\n",
    "        # Insert the data\n",
    "        with Inserter(connection, table_def) as inserter:\n",
    "            inserter.add_rows(unioned_data)\n",
    "            inserter.execute()\n",
    "    print(f\"{time() - start_time}: Done :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all this took 1.134 seconds.\n",
    "While for those rather small Hyper files, the run time is pretty acceptable, it is still slower than it could be.\n",
    "\n",
    "Furthermore, this approach cannot scale to larger datasets.\n",
    "After all, we are keeping all our data in memory within the `unioned_data` array.\n",
    "As soon as your data is larger than a few gigabytes, Python will have serious trouble handling this amount of data.\n",
    "\n",
    "Let's see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The recommended solution: Let Hyper do the work\n",
    "\n",
    "The key insight to a more efficient solution is:\n",
    "\n",
    "*Hyper allows you to work on multiple data base files within the same connection.*\n",
    "\n",
    "Using this capability, we can instruct Hyper to directly move data between various Hyper files without ever moving a single row to Python. And believe me: Hyper is faster at moving data around than Python.\n",
    "\n",
    "So, how do we do we actually tell Hyper to access multiple Hyper files?\n",
    "We do so using the `attach_database` method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tableauhyperapi import HyperProcess, Connection, Telemetry\n",
    "\n",
    "hyper = HyperProcess(Telemetry.SEND_USAGE_DATA_TO_TABLEAU, 'unionfiles_hacking')\n",
    "# Don't pass in any database to connect against.\n",
    "# We want on an \"empty\" connection.\n",
    "connection = Connection(hyper.endpoint)\n",
    "# Let's \"attach\" two databases to our connection\n",
    "connection.catalog.attach_database(\"WorldIndicators_2000.hyper\", alias=\"input1\")\n",
    "connection.catalog.attach_database(\"WorldIndicators_2001.hyper\", alias=\"input2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper now loaded both databases.\n",
    "We can now access both databases through SQL by specifying our tables using the syntax\n",
    "*`<database alias>.<schema name>.<table name>`* where `database alias` is the alias we provided when calling `attach_database`.\n",
    "Let's quickly peek into our two databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Date(2000, 12, 1)]]\n",
      "[[Date(2001, 12, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(connection.execute_list_query('SELECT \"Year\" FROM \"input1\".\"Extract\".\"Extract\" LIMIT 1'))\n",
    "print(connection.execute_list_query('SELECT \"Year\" FROM \"input2\".\"Extract\".\"Extract\" LIMIT 1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks about right ðŸ˜ƒ\n",
    "\n",
    "Next step: Let's use our SQL knowledge and let Hyper directly Union both tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unioned data has 416 tuples\n"
     ]
    }
   ],
   "source": [
    "unioned_data = connection.execute_list_query('''\n",
    "    SELECT * FROM \"input1\".\"Extract\".\"Extract\"\n",
    "    UNION ALL\n",
    "    SELECT * FROM \"input2\".\"Extract\".\"Extract\"\n",
    "    ''')\n",
    "print(f\"unioned data has {len(unioned_data)} tuples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there...\n",
    "\n",
    "Now we would like to store the UNIONed data into a new table *without moving it through Python*.\n",
    "Turns out, this is also possible, thanks to `CREATE TABLE AS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unioned data has 416 tuples\n"
     ]
    }
   ],
   "source": [
    "connection.execute_list_query('''\n",
    "    CREATE TEMPORARY TABLE unioned_data AS\n",
    "    SELECT * FROM \"input1\".\"Extract\".\"Extract\"\n",
    "    UNION ALL\n",
    "    SELECT * FROM \"input2\".\"Extract\".\"Extract\"\n",
    "    ''')\n",
    "unioned_data_len = connection.execute_scalar_query(\n",
    "    'SELECT COUNT(*) FROM unioned_data')\n",
    "print(f\"unioned data has {unioned_data_len} tuples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with those building blocks, we are almost done.\n",
    "All we are missing is persisting the newly created table into a separate Hyper database.\n",
    "\n",
    "But before moving on, let's be polite and clean up after us. Since we aren't using Python's `with` this time, we should close the connection and shutdown the hyper server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()\n",
    "hyper.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now let's jump right into the final version of our little script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4374821186065674: Attached all input databases...\n",
      "\"Extract\".\"Extract\"\n",
      "0.4706258773803711: Prepared output database\n",
      "0.7011048793792725: Done :)\n"
     ]
    }
   ],
   "source": [
    "from tableauhyperapi import HyperProcess, Connection, Telemetry, TableDefinition, TableName, SchemaName, Inserter, CreateMode\n",
    "from glob import glob\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "input_files = glob(\"WorldIndicators_*.hyper\")\n",
    "table_name = TableName('Extract','Extract')\n",
    "output_file = \"WorldIndicatorsMerged.hyper\"\n",
    "\n",
    "# Let's delete the output file to make sure we can rerun this script\n",
    "# even if the output already exists.\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Start a new Hyper instance\n",
    "start_time = time()\n",
    "with HyperProcess(Telemetry.SEND_USAGE_DATA_TO_TABLEAU, 'unionfiles_efficient') as hyper:\n",
    "    # Create a connection without any connected databases\n",
    "    with Connection(hyper.endpoint) as connection:\n",
    "        # Connect to all our input databases\n",
    "        for i, file in enumerate(input_files):\n",
    "            connection.catalog.attach_database(file, alias=f\"input{i}\")\n",
    "        print(f\"{time() - start_time}: Attached all input databases...\")\n",
    "        # Prepare the output database\n",
    "        connection.catalog.create_database(output_file)\n",
    "        connection.catalog.attach_database(output_file, alias=\"output\")\n",
    "        print(table_name)\n",
    "        connection.catalog.create_schema(SchemaName(\"output\", table_name.schema_name))\n",
    "        print(f\"{time() - start_time}: Prepared output database\")\n",
    "        # Build the CREATE TABLE AS command which unions all our inputs\n",
    "        union_query = ' UNION ALL\\n'.join(\n",
    "            f'SELECT * FROM \"input{i}\".{table_name}' for i in range(len(input_files)))\n",
    "        create_table_sql = f'CREATE TABLE \"output\".{table_name} AS \\n{union_query}'\n",
    "        # And execute it\n",
    "        connection.execute_command(create_table_sql)\n",
    "    print(f\"{time() - start_time}: Done :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, we were twice as fast this time!  ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "Given that each of our inputs only had 213 rows, that's pretty impressive.\n",
    "Of course, for larger inputs the difference only gets larger.\n",
    "I am looking forward to hear from your local experiments on hopefully larger files!\n",
    "\n",
    "The code by itself should be pretty much self-explanatory:\n",
    "\n",
    "1. Spawn a HyperProcess and connect to it\n",
    "2. Attach all our input databases to our connection\n",
    "3. Prepare an output file and also attach it to our session\n",
    "4. Format a SQL query which stores the UNIONed results into our new database\n",
    "5. Done :)\n",
    "\n",
    "The formatting step might be a bit hard to read for non-Python devs.\n",
    "For everyone who isn't fluent with Python formatting, this is the generated query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"output\".\"Extract\".\"Extract\" AS \n",
      "SELECT * FROM \"input0\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input1\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input2\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input3\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input4\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input5\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input6\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input7\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input8\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input9\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input10\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input11\".\"Extract\".\"Extract\" UNION ALL\n",
      "SELECT * FROM \"input12\".\"Extract\".\"Extract\"\n"
     ]
    }
   ],
   "source": [
    "print(create_table_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty straight-forward - feel free to reimplement in your favorite language.\n",
    "\n",
    "Although, at least from a performance perspective there isn't really a reason to not just stick with Python here.\n",
    "With this solution, Python is only formatting the queries and sending them to Hyper.\n",
    "The actual work happens in Hyper.\n",
    "Python code never touches the indivual rows which we are copying between the files.\n",
    "And at least for string processing, it really doesn't make a difference if you use Python, C++ or hand-optimized Assembly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this blog post, we looked at two implementations to UNION a set of Hyper files.\n",
    "We first looked into a straight-forward solution which consisted of reading all data from all input files, unioning it in a Python list and then inserting all rows into the output file.\n",
    "\n",
    "This approach has the large drawback, that all data is moved through Python.\n",
    "And it turns out that Python is not exactly the most efficient language for per-row operations.\n",
    "\n",
    "However, Hyper actually allows you to *send SQL commands referencing tables from multiple different Hyper files*.\n",
    "\n",
    "With that capability, we can express our complete UNIONing in a single query.\n",
    "This query takes care of both unioning the data and inserting it into the output table.\n",
    "That way, we make sure that Python never goes over the individual tuples.\n",
    "All tuples stay within Hyper and Hyper turns out to be pretty efficient at processing data rows - which is exactly what Hyper was built for."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
